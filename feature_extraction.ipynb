{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 1583: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 86\u001b[0m\n\u001b[0;32m     83\u001b[0m input_csv \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompiled_output.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Path to your compiled output CSV file\u001b[39;00m\n\u001b[0;32m     84\u001b[0m output_csv \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextracted_features.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Path to the output CSV with features\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m \u001b[43mprocess_and_extract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_csv\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 69\u001b[0m, in \u001b[0;36mprocess_and_extract_features\u001b[1;34m(input_csv, output_csv)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(input_csv, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m infile:\n\u001b[0;32m     68\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(infile)\n\u001b[1;32m---> 69\u001b[0m     header \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreader\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Skip the header row\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(reader)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Open the output CSV to write features\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 1583: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example list of agreement and disagreement lexicons\n",
    "agreement_lexicon = ['agree', 'agreeing', 'support', 'supporting', 'in favor of', 'yes', 'yes', 'certainly', 'definitely', 'sure']\n",
    "disagreement_lexicon = ['disagree', 'disagreeing', 'oppose', 'opposing', 'against', 'no', 'not', 'never', 'certainly not', 'definitely not']\n",
    "\n",
    "# Helper functions\n",
    "\n",
    "# 1. N-gram generation\n",
    "def generate_ngrams(text, n=1):\n",
    "    tokens = nltk.word_tokenize(text.lower())  # Tokenize and lowercase\n",
    "    return list(ngrams(tokens, n))\n",
    "\n",
    "# 2. Extract Modal Verbs using spaCy\n",
    "def extract_modal_verbs(text):\n",
    "    doc = nlp(text)\n",
    "    modal_verbs = [token.text for token in doc if token.tag_ == 'MD']  # Modal verbs in spaCy are tagged as 'MD'\n",
    "    return ' '.join(modal_verbs)\n",
    "\n",
    "# 3. Detect Negations\n",
    "def detect_negation(text):\n",
    "    doc = nlp(text)\n",
    "    negations = [token.text for token in doc if token.dep_ == 'neg']\n",
    "    return ' '.join(negations)\n",
    "\n",
    "# 4. Argument lexicon match (agreement and disagreement)\n",
    "def check_argument_lexicon(text):\n",
    "    tokens = text.lower().split()\n",
    "    agreement_count = sum(1 for word in tokens if word in agreement_lexicon)\n",
    "    disagreement_count = sum(1 for word in tokens if word in disagreement_lexicon)\n",
    "    return agreement_count, disagreement_count\n",
    "\n",
    "# 5. Generate feature vector\n",
    "def extract_features(text, label):\n",
    "    # 1. N-grams (Unigrams, Bigrams, Trigrams)\n",
    "    unigrams = generate_ngrams(text, n=1)\n",
    "    bigrams = generate_ngrams(text, n=2)\n",
    "    trigrams = generate_ngrams(text, n=3)\n",
    "\n",
    "    # Flatten the ngrams into strings\n",
    "    unigram_str = ' '.join(['_'.join(gram) for gram in unigrams])\n",
    "    bigram_str = ' '.join(['_'.join(gram) for gram in bigrams])\n",
    "    trigram_str = ' '.join(['_'.join(gram) for gram in trigrams])\n",
    "\n",
    "    # 2. Argument lexicons (agreement and disagreement)\n",
    "    agreement_count, disagreement_count = check_argument_lexicon(text)\n",
    "\n",
    "    # 3. Modal verbs\n",
    "    modals = extract_modal_verbs(text)\n",
    "\n",
    "    # 4. Negations\n",
    "    negations = detect_negation(text)\n",
    "\n",
    "    # Return the feature vector\n",
    "    return [text, label, unigram_str, bigram_str, trigram_str, agreement_count, disagreement_count, modals, negations]\n",
    "\n",
    "# Function to process compiled_output.csv and extract features\n",
    "def process_and_extract_features(input_csv, output_csv):\n",
    "    with open(input_csv, 'r', encoding='utf-8') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        header = next(reader)  # Skip the header row\n",
    "        data = list(reader)\n",
    "    \n",
    "    # Open the output CSV to write features\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow(['Text', 'Label', 'Unigrams', 'Bigrams', 'Trigrams', 'Agreement_Count', 'Disagreement_Count', 'Modal_Verbs', 'Negations'])\n",
    "        \n",
    "        for row in data:\n",
    "            text, label = row\n",
    "            features = extract_features(text, label)\n",
    "            writer.writerow(features)\n",
    "\n",
    "# Example usage\n",
    "input_csv = 'compiled_output.csv'  # Path to your compiled output CSV file\n",
    "output_csv = 'extracted_features.csv'  # Path to the output CSV with features\n",
    "\n",
    "process_and_extract_features(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
