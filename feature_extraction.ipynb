{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Specify a directory to store NLTK data\n",
    "nltk.data.path.append('C:/nltk_data')  # You can change this path if needed\n",
    "\n",
    "# Download the necessary data\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams: ['real', 'time', 'animation', 'deformable', 'objects', 'compromise']\n",
      "Bigrams: ['real time', 'time animation', 'animation deformable', 'deformable objects', 'objects compromise']\n",
      "Trigrams: ['real time animation', 'time animation deformable', 'animation deformable objects', 'deformable objects compromise']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example list of agreement and disagreement lexicons\n",
    "agreement_lexicon = ['agree', 'agreeing', 'support', 'supporting', 'in favor of', 'yes', 'yes', 'certainly', 'definitely', 'sure']\n",
    "disagreement_lexicon = ['disagree', 'disagreeing', 'oppose', 'opposing', 'against', 'no', 'not', 'never', 'certainly not', 'definitely not']\n",
    "\n",
    "# Helper functions\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def generate_ngrams(text, n=1):\n",
    "    \"\"\"\n",
    "    Tokenizes the text using spaCy and generates n-grams.\n",
    "    \"\"\"\n",
    "    # Tokenize the text using spaCy\n",
    "    doc = nlp(text.lower())  # spaCy tokenization\n",
    "    tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]  # Remove stopwords and punctuation\n",
    "\n",
    "    # Generate n-grams\n",
    "    ngrams_list = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [' '.join(gram) for gram in ngrams_list]\n",
    "\n",
    "# Example usage\n",
    "text = \"Real-time animation of deformable objects is always a compromise.\"\n",
    "unigrams = generate_ngrams(text, 1)\n",
    "bigrams = generate_ngrams(text, 2)\n",
    "trigrams = generate_ngrams(text, 3)\n",
    "\n",
    "print(\"Unigrams:\", unigrams)\n",
    "print(\"Bigrams:\", bigrams)\n",
    "print(\"Trigrams:\", trigrams)\n",
    "\n",
    "\n",
    "# 1. N-gram generation\n",
    "#def generate_ngrams(text, n=1):\n",
    "#    tokens = nltk.word_tokenize(text.lower())  # Tokenize and lowercase\n",
    "#    return list(ngrams(tokens, n))\n",
    "\n",
    "# 2. Extract Modal Verbs using spaCy\n",
    "def extract_modal_verbs(text):\n",
    "    doc = nlp(text)\n",
    "    modal_verbs = [token.text for token in doc if token.tag_ == 'MD']  # Modal verbs in spaCy are tagged as 'MD'\n",
    "    return ' '.join(modal_verbs)\n",
    "\n",
    "# 3. Detect Negations\n",
    "def detect_negation(text):\n",
    "    doc = nlp(text)\n",
    "    negations = [token.text for token in doc if token.dep_ == 'neg']\n",
    "    return ' '.join(negations)\n",
    "\n",
    "# 4. Argument lexicon match (agreement and disagreement)\n",
    "def check_argument_lexicon(text):\n",
    "    tokens = text.lower().split()\n",
    "    agreement_count = sum(1 for word in tokens if word in agreement_lexicon)\n",
    "    disagreement_count = sum(1 for word in tokens if word in disagreement_lexicon)\n",
    "    return agreement_count, disagreement_count\n",
    "\n",
    "# 5. Generate feature vector\n",
    "def extract_features(text, label):\n",
    "    # 1. N-grams (Unigrams, Bigrams, Trigrams)\n",
    "    unigrams = generate_ngrams(text, n=1)\n",
    "    bigrams = generate_ngrams(text, n=2)\n",
    "    trigrams = generate_ngrams(text, n=3)\n",
    "\n",
    "    # Flatten the ngrams into strings\n",
    "    unigram_str = ' '.join([''.join(gram) for gram in unigrams])\n",
    "    bigram_str = ' '.join([''.join(gram) for gram in bigrams])\n",
    "    trigram_str = ' '.join([''.join(gram) for gram in trigrams])\n",
    "\n",
    "    # 2. Argument lexicons (agreement and disagreement)\n",
    "    agreement_count, disagreement_count = check_argument_lexicon(text)\n",
    "\n",
    "    # 3. Modal verbs\n",
    "    modals = extract_modal_verbs(text)\n",
    "\n",
    "    # 4. Negations\n",
    "    negations = detect_negation(text)\n",
    "\n",
    "    # Return the feature vector\n",
    "    return [text, label, unigram_str, bigram_str, trigram_str, agreement_count, disagreement_count, modals, negations]\n",
    "\n",
    "# Function to process compiled_output.csv and extract features\n",
    "def process_and_extract_features(input_csv, output_csv):\n",
    "    with open(input_csv, 'r', encoding='utf-8') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        header = next(reader)  # Skip the header row\n",
    "        data = list(reader)\n",
    "    \n",
    "    # Open the output CSV to write features\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow(['Text', 'Label', 'Unigrams', 'Bigrams', 'Trigrams', 'Agreement_Count', 'Disagreement_Count', 'Modal_Verbs', 'Negations'])\n",
    "        \n",
    "        for row in data:\n",
    "            text, label = row\n",
    "            features = extract_features(text, label)\n",
    "            writer.writerow(features)\n",
    "\n",
    "# Example usage\n",
    "input_csv = 'compiled_output.csv'  # Path to your compiled output CSV file\n",
    "output_csv = 'compiled_features.csv'  # Path to the output CSV with features\n",
    "\n",
    "process_and_extract_features(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
