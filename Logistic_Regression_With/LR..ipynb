{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text             Label\n",
      "0  complicated 3D character models are widely use...  background_claim\n",
      "1  The range of breathtaking realistic 3D models ...  background_claim\n",
      "2         a production cannot afford major revisions  background_claim\n",
      "3  providing a flexible and efficient solution to...         own_claim\n",
      "4  Skeleton Subspace Deformation (SSD) is the pre...  background_claim\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"compiled_output.csv\")\n",
    "\n",
    "# Display the first few rows to check the structure\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing or empty text\n",
    "df['Text'].isnull().sum()  # Check for NaNs\n",
    "df[df['Text'].str.strip() == '']  # Check for empty or all-whitespace texts\n",
    "# Remove rows where text is empty or null\n",
    "df = df[df['Text'].str.strip() != '']\n",
    "df = df.dropna(subset=['Text'])\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words='english')  # Remove stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: complicated 3D character models are widely used in fields of entertainment, virtual reality, medicine etc\n",
      "Tokenized Text: ['complicated', '3D', 'character', 'models', 'are', 'widely', 'used', 'in', 'fields', 'of', 'entertainment,', 'virtual', 'reality,', 'medicine', 'etc']\n"
     ]
    }
   ],
   "source": [
    "# Test tokenization of a sample text\n",
    "sample_text = df['Text'].iloc[0]\n",
    "print(\"Original Text:\", sample_text)\n",
    "print(\"Tokenized Text:\", sample_text.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['3d' '3d character' '3d character models' '3d models' '3d models limited'\n",
      " 'afford' 'afford major' 'afford major revisions' 'animation'\n",
      " 'animation remains' 'animation remains open' 'approach'\n",
      " 'approach character' 'approach character skinning' 'artists'\n",
      " 'artists resolution' 'artists resolution devices' 'breathtaking'\n",
      " 'breathtaking realistic' 'breathtaking realistic 3d' 'character'\n",
      " 'character models' 'character models widely' 'character skinning'\n",
      " 'character skinning present' 'complicated' 'complicated 3d'\n",
      " 'complicated 3d character' 'creativity' 'creativity artists'\n",
      " 'creativity artists resolution' 'deformation' 'deformation ssd'\n",
      " 'deformation ssd predominant' 'devices' 'efficient' 'efficient solution'\n",
      " 'efficient solution animation' 'entertainment' 'entertainment virtual'\n",
      " 'entertainment virtual reality' 'fields' 'fields entertainment'\n",
      " 'fields entertainment virtual' 'flexible' 'flexible efficient'\n",
      " 'flexible efficient solution' 'limited' 'limited creativity'\n",
      " 'limited creativity artists' 'major' 'major revisions' 'medicine'\n",
      " 'models' 'models limited' 'models limited creativity' 'models widely'\n",
      " 'models widely used' 'open' 'open problem' 'predominant'\n",
      " 'predominant approach' 'predominant approach character' 'present'\n",
      " 'problem' 'production' 'production afford' 'production afford major'\n",
      " 'providing' 'providing flexible' 'providing flexible efficient' 'range'\n",
      " 'range breathtaking' 'range breathtaking realistic' 'realistic'\n",
      " 'realistic 3d' 'realistic 3d models' 'reality' 'reality medicine'\n",
      " 'remains' 'remains open' 'remains open problem' 'resolution'\n",
      " 'resolution devices' 'revisions' 'skeleton' 'skeleton subspace'\n",
      " 'skeleton subspace deformation' 'skinning' 'skinning present' 'solution'\n",
      " 'solution animation' 'solution animation remains' 'ssd' 'ssd predominant'\n",
      " 'ssd predominant approach' 'subspace' 'subspace deformation'\n",
      " 'subspace deformation ssd' 'used' 'used fields'\n",
      " 'used fields entertainment' 'virtual' 'virtual reality'\n",
      " 'virtual reality medicine' 'widely' 'widely used' 'widely used fields']\n"
     ]
    }
   ],
   "source": [
    "# Test extracting n-grams from a small set of text\n",
    "sample_texts = df['Text'].head(5)  # First 5 rows for quick testing\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words='english')\n",
    "X_test = vectorizer.fit_transform(sample_texts)\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Argument lexicons\n",
    "agreement_lexicon = ['agree', 'yes', 'definitely', 'sure', 'absolutely', 'of course']\n",
    "disagreement_lexicon = ['disagree', 'no', 'never', 'not', 'don’t', 'won’t']\n",
    "\n",
    "# Hedge words\n",
    "hedge_words = ['perhaps', 'maybe', 'possibly', 'could', 'might', 'probably']\n",
    "\n",
    "# Modal verbs list\n",
    "modal_verbs = ['can', 'could', 'may', 'might', 'shall', 'should', 'will', 'would']\n",
    "\n",
    "# Function to extract n-grams\n",
    "def extract_ngrams(text, ngram_range=(1, 3)):\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "    ngrams = vectorizer.fit_transform([text])\n",
    "    return vectorizer.get_feature_names_out()\n",
    "\n",
    "# Extract Argument Lexicons (Agreement and Disagreement)\n",
    "def extract_argument_lexicon_features(text):\n",
    "    agreement_count = sum([word in text.lower() for word in agreement_lexicon])\n",
    "    disagreement_count = sum([word in text.lower() for word in disagreement_lexicon])\n",
    "    return agreement_count, disagreement_count\n",
    "\n",
    "# Extract Hedge Features\n",
    "def extract_hedge_features(text):\n",
    "    hedge_count = sum([word in text.lower() for word in hedge_words])\n",
    "    return hedge_count\n",
    "\n",
    "# Extract Modal Verbs\n",
    "def extract_modal_verbs(text):\n",
    "    doc = nlp(text)\n",
    "    modal_count = sum([token.lemma_ in modal_verbs for token in doc])\n",
    "    return modal_count\n",
    "\n",
    "# Detect Negation\n",
    "def detect_negation(text):\n",
    "    negation_patterns = [r'\\b(not|no|never|don\\'t|won\\'t|isn\\'t|aren\\'t|can\\'t)\\b']\n",
    "    negation_count = sum([bool(re.search(pattern, text.lower())) for pattern in negation_patterns])\n",
    "    return negation_count\n",
    "\n",
    "# Function to extract all features for each text\n",
    "def extract_features(df):\n",
    "    all_ngrams = []\n",
    "    other_features = []\n",
    "    \n",
    "    for text in df['Text']:\n",
    "        # Extract n-grams\n",
    "        ngrams = extract_ngrams(text)\n",
    "        ngram_features = list(ngrams)  # Get the ngram features\n",
    "        \n",
    "        # Extract argument lexicons (agreement, disagreement)\n",
    "        agreement_count, disagreement_count = extract_argument_lexicon_features(text)\n",
    "\n",
    "        # Extract hedge features\n",
    "        hedge_count = extract_hedge_features(text)\n",
    "\n",
    "        # Extract modal verbs\n",
    "        modal_count = extract_modal_verbs(text)\n",
    "\n",
    "        # Extract negation features\n",
    "        negation_count = detect_negation(text)\n",
    "\n",
    "        # Combine all the features into one list\n",
    "        other_features.append([agreement_count, disagreement_count, hedge_count, modal_count, negation_count])\n",
    "\n",
    "        # Store the ngram features as a separate part of the feature matrix\n",
    "        all_ngrams.append(ngram_features)\n",
    "    \n",
    "    return all_ngrams, other_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract n-grams\n",
    "def extract_ngrams(text, ngram_range=(1, 3)):\n",
    "    if text.strip():  # Check if text is non-empty and not just whitespace\n",
    "        vectorizer = CountVectorizer(ngram_range=ngram_range, stop_words='english')\n",
    "        ngrams = vectorizer.fit_transform([text])\n",
    "        return vectorizer.get_feature_names_out()\n",
    "    else:\n",
    "        return []  # Return an empty list for empty text\n",
    "\n",
    "# Extract n-grams and other features\n",
    "def extract_features(df):\n",
    "    all_ngrams = []\n",
    "    other_features = []\n",
    "    \n",
    "    for text in df['Text']:\n",
    "        # Extract n-grams\n",
    "        ngrams = extract_ngrams(text)\n",
    "        ngram_features = list(ngrams)  # Get the ngram features\n",
    "        \n",
    "        # Extract argument lexicons (agreement, disagreement)\n",
    "        agreement_count, disagreement_count = extract_argument_lexicon_features(text)\n",
    "\n",
    "        # Extract hedge features\n",
    "        hedge_count = extract_hedge_features(text)\n",
    "\n",
    "        # Extract modal verbs\n",
    "        modal_count = extract_modal_verbs(text)\n",
    "\n",
    "        # Extract negation features\n",
    "        negation_count = detect_negation(text)\n",
    "\n",
    "        # Combine all the features into one list\n",
    "        other_features.append([agreement_count, disagreement_count, hedge_count, modal_count, negation_count])\n",
    "\n",
    "        # Store the ngram features as a separate part of the feature matrix\n",
    "        all_ngrams.append(ngram_features)\n",
    "    \n",
    "    return all_ngrams, other_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract n-grams\n",
    "def extract_ngrams(text, ngram_range=(1, 3)):\n",
    "    if text.strip():  # Check if text is non-empty and not just whitespace\n",
    "        vectorizer = CountVectorizer(ngram_range=ngram_range, stop_words='english')\n",
    "        ngrams = vectorizer.fit_transform([text])\n",
    "        return vectorizer.get_feature_names_out()\n",
    "    else:\n",
    "        return []  # Return an empty list for empty text\n",
    "\n",
    "# Extract n-grams and other features\n",
    "def extract_features(df):\n",
    "    all_ngrams = []\n",
    "    other_features = []\n",
    "    \n",
    "    for text in df['Text']:\n",
    "        # Extract n-grams\n",
    "        ngrams = extract_ngrams(text)\n",
    "        ngram_features = list(ngrams)  # Get the ngram features\n",
    "        \n",
    "        # Extract argument lexicons (agreement, disagreement)\n",
    "        agreement_count, disagreement_count = extract_argument_lexicon_features(text)\n",
    "\n",
    "        # Extract hedge features\n",
    "        hedge_count = extract_hedge_features(text)\n",
    "\n",
    "        # Extract modal verbs\n",
    "        modal_count = extract_modal_verbs(text)\n",
    "\n",
    "        # Extract negation features\n",
    "        negation_count = detect_negation(text)\n",
    "\n",
    "        # Combine all the features into one list\n",
    "        other_features.append([agreement_count, disagreement_count, hedge_count, modal_count, negation_count])\n",
    "\n",
    "        # Store the ngram features as a separate part of the feature matrix\n",
    "        all_ngrams.append(ngram_features)\n",
    "    \n",
    "    return all_ngrams, other_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Argument lexicons\n",
    "agreement_lexicon = ['agree', 'yes', 'definitely', 'sure', 'absolutely', 'of course']\n",
    "disagreement_lexicon = ['disagree', 'no', 'never', 'not', 'don’t', 'won’t']\n",
    "\n",
    "# Hedge words\n",
    "hedge_words = ['perhaps', 'maybe', 'possibly', 'could', 'might', 'probably']\n",
    "\n",
    "# Modal verbs list\n",
    "modal_verbs = ['can', 'could', 'may', 'might', 'shall', 'should', 'will', 'would']\n",
    "\n",
    "# Function to extract n-grams\n",
    "def extract_ngrams(text, ngram_range=(1, 3)):\n",
    "    if text.strip():  # Check if text is non-empty and not just whitespace\n",
    "        vectorizer = CountVectorizer(ngram_range=ngram_range, stop_words='english')\n",
    "        ngrams = vectorizer.fit_transform([text])\n",
    "        return vectorizer.get_feature_names_out()\n",
    "    else:\n",
    "        return []  # Return an empty list for empty text\n",
    "\n",
    "# Extract Argument Lexicons (Agreement and Disagreement)\n",
    "def extract_argument_lexicon_features(text):\n",
    "    agreement_count = sum([word in text.lower() for word in agreement_lexicon])\n",
    "    disagreement_count = sum([word in text.lower() for word in disagreement_lexicon])\n",
    "    return agreement_count, disagreement_count\n",
    "\n",
    "# Extract Hedge Features\n",
    "def extract_hedge_features(text):\n",
    "    hedge_count = sum([word in text.lower() for word in hedge_words])\n",
    "    return hedge_count\n",
    "\n",
    "# Extract Modal Verbs\n",
    "def extract_modal_verbs(text):\n",
    "    doc = nlp(text)\n",
    "    modal_count = sum([token.lemma_ in modal_verbs for token in doc])\n",
    "    return modal_count\n",
    "\n",
    "# Detect Negation\n",
    "def detect_negation(text):\n",
    "    negation_patterns = [r'\\b(not|no|never|don\\'t|won\\'t|isn\\'t|aren\\'t|can\\'t)\\b']\n",
    "    negation_count = sum([bool(re.search(pattern, text.lower())) for pattern in negation_patterns])\n",
    "    return negation_count\n",
    "\n",
    "# Function to extract all features for each text\n",
    "def extract_features(df):\n",
    "    all_ngrams = []\n",
    "    other_features = []\n",
    "    \n",
    "    for text in df['Text']:\n",
    "        # Extract n-grams\n",
    "        ngrams = extract_ngrams(text)\n",
    "        ngram_features = list(ngrams)  # Get the ngram features\n",
    "        \n",
    "        # Extract argument lexicons (agreement, disagreement)\n",
    "        agreement_count, disagreement_count = extract_argument_lexicon_features(text)\n",
    "\n",
    "        # Extract hedge features\n",
    "        hedge_count = extract_hedge_features(text)\n",
    "\n",
    "        # Extract modal verbs\n",
    "        modal_count = extract_modal_verbs(text)\n",
    "\n",
    "        # Extract negation features\n",
    "        negation_count = detect_negation(text)\n",
    "\n",
    "        # Combine all the features into one list\n",
    "        other_features.append([agreement_count, disagreement_count, hedge_count, modal_count, negation_count])\n",
    "\n",
    "        # Store the ngram features as a separate part of the feature matrix\n",
    "        all_ngrams.append(ngram_features)\n",
    "    \n",
    "    return all_ngrams, other_features\n",
    "\n",
    "# Now, proceed with the previous steps to create feature matrix and train Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Extract features and labels\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#df = df[df['Text'].notna() & (df['Text'].str.strip() != '')]\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m ngram_features, additional_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Combine n-grams and additional features into a single feature matrix\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# N-grams will be a list of list of n-grams, so we need to flatten this into a single feature vector for each entry\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n",
      "Cell \u001b[1;32mIn[31], line 17\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     13\u001b[0m other_features \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Extract n-grams\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     ngrams \u001b[38;5;241m=\u001b[39m \u001b[43mextract_ngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     ngram_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ngrams)  \u001b[38;5;66;03m# Get the ngram features\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Extract argument lexicons (agreement, disagreement)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[31], line 5\u001b[0m, in \u001b[0;36mextract_ngrams\u001b[1;34m(text, ngram_range)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text\u001b[38;5;241m.\u001b[39mstrip():  \u001b[38;5;66;03m# Check if text is non-empty and not just whitespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(ngram_range\u001b[38;5;241m=\u001b[39mngram_range, stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     ngrams \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1278\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1279\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1280\u001b[0m         )\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract features and labels\n",
    "#df = df[df['Text'].notna() & (df['Text'].str.strip() != '')]\n",
    "ngram_features, additional_features = extract_features(df)\n",
    "\n",
    "# Combine n-grams and additional features into a single feature matrix\n",
    "# N-grams will be a list of list of n-grams, so we need to flatten this into a single feature vector for each entry\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Combine n-grams into one string for each row to pass into vectorizer\n",
    "ngram_texts = [' '.join(ngram) for ngram in ngram_features]\n",
    "\n",
    "# Create the CountVectorizer for n-grams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words='english')\n",
    "X_ngrams = vectorizer.fit_transform(ngram_texts)\n",
    "\n",
    "# Convert additional features to a numpy array\n",
    "X_additional = np.array(additional_features)\n",
    "\n",
    "# Combine n-grams and additional features (horizontal stack)\n",
    "from scipy.sparse import hstack\n",
    "X = hstack([X_ngrams, X_additional])\n",
    "\n",
    "# Prepare the target labels (Y)\n",
    "y = df['Label'].values\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "count    13592.000000\n",
      "mean        59.892216\n",
      "std         45.405939\n",
      "min          1.000000\n",
      "25%         22.000000\n",
      "50%         53.000000\n",
      "75%         86.000000\n",
      "max        359.000000\n",
      "Name: Text, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df['Text'].isnull().sum())  # Check for any null values\n",
    "print(df['Text'].apply(len).describe()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text             Label\n",
      "0  complicated 3D character models are widely use...  background_claim\n",
      "1  The range of breathtaking realistic 3D models ...  background_claim\n",
      "2         a production cannot afford major revisions  background_claim\n",
      "3  providing a flexible and efficient solution to...         own_claim\n",
      "4  Skeleton Subspace Deformation (SSD) is the pre...  background_claim\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 88\u001b[0m\n\u001b[0;32m     85\u001b[0m df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText_Length\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Extract features and labels\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m ngram_features, additional_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Combine n-grams into one string for each row to pass into vectorizer\u001b[39;00m\n\u001b[0;32m     91\u001b[0m ngram_texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ngram) \u001b[38;5;28;01mfor\u001b[39;00m ngram \u001b[38;5;129;01min\u001b[39;00m ngram_features]\n",
      "Cell \u001b[1;32mIn[2], line 60\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Loop through each text entry in the dataframe\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# Extract n-grams (unigrams, bigrams, trigrams)\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     ngrams \u001b[38;5;241m=\u001b[39m \u001b[43mextract_ngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# Extract additional features\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     argument_lexicons \u001b[38;5;241m=\u001b[39m extract_argument_lexicons(text)\n",
      "Cell \u001b[1;32mIn[2], line 19\u001b[0m, in \u001b[0;36mextract_ngrams\u001b[1;34m(text, ngram_range)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text\u001b[38;5;241m.\u001b[39mstrip():\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Initialize vectorizer without stopwords\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(ngram_range\u001b[38;5;241m=\u001b[39mngram_range, stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 19\u001b[0m     ngrams \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# If ngrams contain valid features, return them, otherwise return empty\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ngrams\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1278\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1279\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1280\u001b[0m         )\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import spacy\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Load the Spacy model for modal verb detection\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define function for extracting n-grams (unigrams, bigrams, trigrams)\n",
    "def extract_ngrams(text, ngram_range=(1, 3)):\n",
    "    # Ensure the text is not empty or consisting of just spaces\n",
    "    if text.strip():\n",
    "        # Initialize vectorizer without stopwords\n",
    "        vectorizer = CountVectorizer(ngram_range=ngram_range, stop_words=None)\n",
    "        ngrams = vectorizer.fit_transform([text])\n",
    "        \n",
    "        # If ngrams contain valid features, return them, otherwise return empty\n",
    "        if ngrams.shape[1] > 0:\n",
    "            return vectorizer.get_feature_names_out()\n",
    "        else:\n",
    "            return []  # Return empty list if no n-grams are found\n",
    "    else:\n",
    "        return []  # Return empty list for empty or invalid text\n",
    "\n",
    "# Example lexicons for argument extraction (these could be refined further)\n",
    "agreement_lexicon = ['agree', 'agreed', 'agreement', 'yes', 'support']\n",
    "disagreement_lexicon = ['disagree', 'disagreed', 'disagreement', 'no', 'oppose']\n",
    "\n",
    "# Function to extract argument lexicons (agreement and disagreement)\n",
    "def extract_argument_lexicons(text):\n",
    "    agreement_count = sum([word in text.lower() for word in agreement_lexicon])\n",
    "    disagreement_count = sum([word in text.lower() for word in disagreement_lexicon])\n",
    "    return [agreement_count, disagreement_count]\n",
    "\n",
    "# Function to detect modal verbs (e.g., \"can\", \"could\", \"will\", etc.)\n",
    "def extract_modal_verbs(text):\n",
    "    doc = nlp(text)\n",
    "    modal_verbs = ['can', 'could', 'will', 'would', 'shall', 'should', 'may', 'might', 'must', 'ought']\n",
    "    modal_count = sum([token.lemma_ in modal_verbs for token in doc])\n",
    "    return [modal_count]\n",
    "\n",
    "# Function to detect negation words (e.g., \"not\", \"never\", etc.)\n",
    "def extract_negation(text):\n",
    "    negation_words = ['not', 'never', 'no', 'none', 'nothing', 'neither', 'nor']\n",
    "    negation_count = sum([word in text.lower() for word in negation_words])\n",
    "    return [negation_count]\n",
    "\n",
    "# Function to extract all features\n",
    "def extract_features(df):\n",
    "    ngram_features = []\n",
    "    other_features = []\n",
    "\n",
    "    # Loop through each text entry in the dataframe\n",
    "    for text in df['Text']:\n",
    "        # Extract n-grams (unigrams, bigrams, trigrams)\n",
    "        ngrams = extract_ngrams(text)\n",
    "        \n",
    "        # Extract additional features\n",
    "        argument_lexicons = extract_argument_lexicons(text)\n",
    "        modal_verbs = extract_modal_verbs(text)\n",
    "        negation = extract_negation(text)\n",
    "        \n",
    "        # Combine all features into a single list for this text\n",
    "        ngram_features.append(ngrams)\n",
    "        other_features.append(argument_lexicons + modal_verbs + negation)\n",
    "    \n",
    "    return ngram_features, other_features\n",
    "\n",
    "# Read the data from the CSV file\n",
    "df = pd.read_csv('compiled_output.csv')  # Replace with your file path\n",
    "\n",
    "# Check the first few rows of the dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Remove rows with NaN or empty Text after stripping extra spaces\n",
    "df = df[df['Text'].notna() & (df['Text'].str.strip() != '')]\n",
    "df['Text'] = df['Text'].str.strip()  # Remove leading/trailing spaces\n",
    "\n",
    "# Remove rows with empty or very short texts (length < 3 words)\n",
    "df['Text_Length'] = df['Text'].apply(lambda x: len(x.split()))\n",
    "df = df[df['Text_Length'] > 2]\n",
    "\n",
    "# Extract features and labels\n",
    "ngram_features, additional_features = extract_features(df)\n",
    "\n",
    "# Combine n-grams into one string for each row to pass into vectorizer\n",
    "ngram_texts = [' '.join(ngram) for ngram in ngram_features]\n",
    "\n",
    "# Create the CountVectorizer for n-grams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words=None)\n",
    "X_ngrams = vectorizer.fit_transform(ngram_texts)\n",
    "\n",
    "# Convert additional features to a numpy array\n",
    "X_additional = np.array(additional_features)\n",
    "\n",
    "# Combine n-grams and additional features (horizontal stack)\n",
    "X = hstack([X_ngrams, X_additional])\n",
    "\n",
    "# Prepare the target labels (Y)\n",
    "y = df['Label'].values\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text             Label\n",
      "0  complicated 3D character models are widely use...  background_claim\n",
      "1  The range of breathtaking realistic 3D models ...  background_claim\n",
      "2         a production cannot afford major revisions  background_claim\n",
      "3  providing a flexible and efficient solution to...         own_claim\n",
      "4  Skeleton Subspace Deformation (SSD) is the pre...  background_claim\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 86\u001b[0m\n\u001b[0;32m     83\u001b[0m df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText_Length\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Extract features and labels\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m ngram_features, additional_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Combine n-grams into one string for each row to pass into vectorizer\u001b[39;00m\n\u001b[0;32m     89\u001b[0m ngram_texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ngram) \u001b[38;5;28;01mfor\u001b[39;00m ngram \u001b[38;5;129;01min\u001b[39;00m ngram_features]\n",
      "Cell \u001b[1;32mIn[3], line 58\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Loop through each text entry in the dataframe\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# Extract n-grams (unigrams, bigrams, trigrams)\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m     ngrams \u001b[38;5;241m=\u001b[39m \u001b[43mextract_ngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# Extract additional features\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     argument_lexicons \u001b[38;5;241m=\u001b[39m extract_argument_lexicons(text)\n",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m, in \u001b[0;36mextract_ngrams\u001b[1;34m(text, ngram_range)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text\u001b[38;5;241m.\u001b[39mstrip():  \u001b[38;5;66;03m# Ensure the text is not empty or just whitespace\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(ngram_range\u001b[38;5;241m=\u001b[39mngram_range, stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m     ngrams \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# If ngrams contain valid features, return them, otherwise return empty\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ngrams\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1278\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1279\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1280\u001b[0m         )\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import spacy\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Load the Spacy model for modal verb detection\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define function for extracting n-grams (unigrams, bigrams, trigrams)\n",
    "def extract_ngrams(text, ngram_range=(1, 3)):\n",
    "    if text.strip():  # Ensure the text is not empty or just whitespace\n",
    "        vectorizer = CountVectorizer(ngram_range=ngram_range, stop_words='english')\n",
    "        ngrams = vectorizer.fit_transform([text])\n",
    "        \n",
    "        # If ngrams contain valid features, return them, otherwise return empty\n",
    "        if ngrams.shape[1] > 0:\n",
    "            return vectorizer.get_feature_names_out()\n",
    "        else:\n",
    "            return []  # Return empty list if no n-grams are found\n",
    "    else:\n",
    "        return []  # Return empty list for empty or invalid text\n",
    "\n",
    "# Example lexicons for argument extraction (these could be refined further)\n",
    "agreement_lexicon = ['agree', 'agreed', 'agreement', 'yes', 'support']\n",
    "disagreement_lexicon = ['disagree', 'disagreed', 'disagreement', 'no', 'oppose']\n",
    "\n",
    "# Function to extract argument lexicons (agreement and disagreement)\n",
    "def extract_argument_lexicons(text):\n",
    "    agreement_count = sum([word in text.lower() for word in agreement_lexicon])\n",
    "    disagreement_count = sum([word in text.lower() for word in disagreement_lexicon])\n",
    "    return [agreement_count, disagreement_count]\n",
    "\n",
    "# Function to detect modal verbs (e.g., \"can\", \"could\", \"will\", etc.)\n",
    "def extract_modal_verbs(text):\n",
    "    doc = nlp(text)\n",
    "    modal_verbs = ['can', 'could', 'will', 'would', 'shall', 'should', 'may', 'might', 'must', 'ought']\n",
    "    modal_count = sum([token.lemma_ in modal_verbs for token in doc])\n",
    "    return [modal_count]\n",
    "\n",
    "# Function to detect negation words (e.g., \"not\", \"never\", etc.)\n",
    "def extract_negation(text):\n",
    "    negation_words = ['not', 'never', 'no', 'none', 'nothing', 'neither', 'nor']\n",
    "    negation_count = sum([word in text.lower() for word in negation_words])\n",
    "    return [negation_count]\n",
    "\n",
    "# Function to extract all features\n",
    "def extract_features(df):\n",
    "    ngram_features = []\n",
    "    other_features = []\n",
    "\n",
    "    # Loop through each text entry in the dataframe\n",
    "    for text in df['Text']:\n",
    "        # Extract n-grams (unigrams, bigrams, trigrams)\n",
    "        ngrams = extract_ngrams(text)\n",
    "        \n",
    "        # Extract additional features\n",
    "        argument_lexicons = extract_argument_lexicons(text)\n",
    "        modal_verbs = extract_modal_verbs(text)\n",
    "        negation = extract_negation(text)\n",
    "        \n",
    "        # Combine all features into a single list for this text\n",
    "        ngram_features.append(ngrams)\n",
    "        other_features.append(argument_lexicons + modal_verbs + negation)\n",
    "    \n",
    "    return ngram_features, other_features\n",
    "\n",
    "# Read the data from the CSV file\n",
    "df = pd.read_csv('compiled_output.csv')  # Replace with your file path\n",
    "\n",
    "# Check the first few rows of the dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Remove rows with NaN or empty Text after stripping extra spaces\n",
    "df = df[df['Text'].notna() & (df['Text'].str.strip() != '')]\n",
    "df['Text'] = df['Text'].str.strip()  # Remove leading/trailing spaces\n",
    "\n",
    "# Remove rows with empty or very short texts (length < 3 words)\n",
    "df['Text_Length'] = df['Text'].apply(lambda x: len(x.split()))\n",
    "df = df[df['Text_Length'] > 2]\n",
    "\n",
    "# Extract features and labels\n",
    "ngram_features, additional_features = extract_features(df)\n",
    "\n",
    "# Combine n-grams into one string for each row to pass into vectorizer\n",
    "ngram_texts = [' '.join(ngram) for ngram in ngram_features]\n",
    "\n",
    "# Create the CountVectorizer for n-grams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words='english')  # Added stop_words='english' here\n",
    "X_ngrams = vectorizer.fit_transform(ngram_texts)\n",
    "\n",
    "# Convert additional features to a numpy array\n",
    "X_additional = np.array(additional_features)\n",
    "\n",
    "# Combine n-grams and additional features (horizontal stack)\n",
    "X = hstack([X_ngrams, X_additional])\n",
    "\n",
    "# Prepare the target labels (Y)\n",
    "y = df['Label'].values\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text             Label\n",
      "0  complicated 3D character models are widely use...  background_claim\n",
      "1  The range of breathtaking realistic 3D models ...  background_claim\n",
      "2         a production cannot afford major revisions  background_claim\n",
      "3  providing a flexible and efficient solution to...         own_claim\n",
      "4  Skeleton Subspace Deformation (SSD) is the pre...  background_claim\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 86\u001b[0m\n\u001b[0;32m     83\u001b[0m df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText_Length\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Extract features and labels\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m ngram_features, additional_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Combine n-grams into one string for each row to pass into vectorizer\u001b[39;00m\n\u001b[0;32m     89\u001b[0m ngram_texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ngram) \u001b[38;5;28;01mfor\u001b[39;00m ngram \u001b[38;5;129;01min\u001b[39;00m ngram_features]\n",
      "Cell \u001b[1;32mIn[4], line 58\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Loop through each text entry in the dataframe\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# Extract n-grams (unigrams, bigrams, trigrams)\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m     ngrams \u001b[38;5;241m=\u001b[39m \u001b[43mextract_ngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# Extract additional features\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     argument_lexicons \u001b[38;5;241m=\u001b[39m extract_argument_lexicons(text)\n",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m, in \u001b[0;36mextract_ngrams\u001b[1;34m(text, ngram_range)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text\u001b[38;5;241m.\u001b[39mstrip():  \u001b[38;5;66;03m# Ensure the text is not empty or just whitespace\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(ngram_range\u001b[38;5;241m=\u001b[39mngram_range, stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m     ngrams \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# If ngrams contain valid features, return them, otherwise return empty\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ngrams\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1278\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1279\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1280\u001b[0m         )\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import spacy\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Load the Spacy model for modal verb detection\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define function for extracting n-grams (unigrams, bigrams, trigrams)\n",
    "def extract_ngrams(text, ngram_range=(1, 3)):\n",
    "    if text.strip():  # Ensure the text is not empty or just whitespace\n",
    "        vectorizer = CountVectorizer(ngram_range=ngram_range, stop_words='english')\n",
    "        ngrams = vectorizer.fit_transform([text])\n",
    "        \n",
    "        # If ngrams contain valid features, return them, otherwise return empty\n",
    "        if ngrams.shape[1] > 0:\n",
    "            return vectorizer.get_feature_names_out()\n",
    "        else:\n",
    "            return []  # Return empty list if no n-grams are found\n",
    "    else:\n",
    "        return []  # Return empty list for empty or invalid text\n",
    "\n",
    "# Example lexicons for argument extraction (these could be refined further)\n",
    "agreement_lexicon = ['agree', 'agreed', 'agreement', 'yes', 'support']\n",
    "disagreement_lexicon = ['disagree', 'disagreed', 'disagreement', 'no', 'oppose']\n",
    "\n",
    "# Function to extract argument lexicons (agreement and disagreement)\n",
    "def extract_argument_lexicons(text):\n",
    "    agreement_count = sum([word in text.lower() for word in agreement_lexicon])\n",
    "    disagreement_count = sum([word in text.lower() for word in disagreement_lexicon])\n",
    "    return [agreement_count, disagreement_count]\n",
    "\n",
    "# Function to detect modal verbs (e.g., \"can\", \"could\", \"will\", etc.)\n",
    "def extract_modal_verbs(text):\n",
    "    doc = nlp(text)\n",
    "    modal_verbs = ['can', 'could', 'will', 'would', 'shall', 'should', 'may', 'might', 'must', 'ought']\n",
    "    modal_count = sum([token.lemma_ in modal_verbs for token in doc])\n",
    "    return [modal_count]\n",
    "\n",
    "# Function to detect negation words (e.g., \"not\", \"never\", etc.)\n",
    "def extract_negation(text):\n",
    "    negation_words = ['not', 'never', 'no', 'none', 'nothing', 'neither', 'nor']\n",
    "    negation_count = sum([word in text.lower() for word in negation_words])\n",
    "    return [negation_count]\n",
    "\n",
    "# Function to extract all features\n",
    "def extract_features(df):\n",
    "    ngram_features = []\n",
    "    other_features = []\n",
    "\n",
    "    # Loop through each text entry in the dataframe\n",
    "    for text in df['Text']:\n",
    "        # Extract n-grams (unigrams, bigrams, trigrams)\n",
    "        ngrams = extract_ngrams(text)\n",
    "        \n",
    "        # Extract additional features\n",
    "        argument_lexicons = extract_argument_lexicons(text)\n",
    "        modal_verbs = extract_modal_verbs(text)\n",
    "        negation = extract_negation(text)\n",
    "        \n",
    "        # Combine all features into a single list for this text\n",
    "        ngram_features.append(ngrams)\n",
    "        other_features.append(argument_lexicons + modal_verbs + negation)\n",
    "    \n",
    "    return ngram_features, other_features\n",
    "\n",
    "# Read the data from the CSV file\n",
    "df = pd.read_csv('compiled_output.csv')  # Replace with your file path\n",
    "\n",
    "# Check the first few rows of the dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Remove rows with NaN or empty Text after stripping extra spaces\n",
    "df = df[df['Text'].notna() & (df['Text'].str.strip() != '')]\n",
    "df['Text'] = df['Text'].str.strip()  # Remove leading/trailing spaces\n",
    "\n",
    "# Remove rows with empty or very short texts (length < 3 words)\n",
    "df['Text_Length'] = df['Text'].apply(lambda x: len(x.split()))\n",
    "df = df[df['Text_Length'] > 2]\n",
    "\n",
    "# Extract features and labels\n",
    "ngram_features, additional_features = extract_features(df)\n",
    "\n",
    "# Combine n-grams into one string for each row to pass into vectorizer\n",
    "ngram_texts = [' '.join(ngram) for ngram in ngram_features]\n",
    "\n",
    "# Create the CountVectorizer for n-grams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words='english')  # Added stop_words='english' here\n",
    "X_ngrams = vectorizer.fit_transform(ngram_texts)\n",
    "\n",
    "# Convert additional features to a numpy array\n",
    "X_additional = np.array(additional_features)\n",
    "\n",
    "# Combine n-grams and additional features (horizontal stack)\n",
    "X = hstack([X_ngrams, X_additional])\n",
    "\n",
    "# Prepare the target labels (Y)\n",
    "y = df['Label'].values\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text             Label\n",
      "0  complicated 3D character models are widely use...  background_claim\n",
      "1  The range of breathtaking realistic 3D models ...  background_claim\n",
      "2         a production cannot afford major revisions  background_claim\n",
      "3  providing a flexible and efficient solution to...         own_claim\n",
      "4  Skeleton Subspace Deformation (SSD) is the pre...  background_claim\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 87\u001b[0m\n\u001b[0;32m     84\u001b[0m df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText_Length\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Extract features and labels\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m ngram_features, additional_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Combine n-grams into one string for each row to pass into vectorizer\u001b[39;00m\n\u001b[0;32m     90\u001b[0m ngram_texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ngram) \u001b[38;5;28;01mfor\u001b[39;00m ngram \u001b[38;5;129;01min\u001b[39;00m ngram_features]\n",
      "Cell \u001b[1;32mIn[5], line 59\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Loop through each text entry in the dataframe\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# Extract n-grams (unigrams, bigrams, trigrams)\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m     ngrams \u001b[38;5;241m=\u001b[39m \u001b[43mextract_ngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# Extract additional features\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     argument_lexicons \u001b[38;5;241m=\u001b[39m extract_argument_lexicons(text)\n",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m, in \u001b[0;36mextract_ngrams\u001b[1;34m(text, ngram_range)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text\u001b[38;5;241m.\u001b[39mstrip():  \n\u001b[0;32m     17\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(ngram_range\u001b[38;5;241m=\u001b[39mngram_range, stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m     ngrams \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# If no valid n-grams are found, return empty list\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ngrams\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1278\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1279\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1280\u001b[0m         )\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import spacy\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Load the Spacy model for modal verb detection\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define function for extracting n-grams (unigrams, bigrams, trigrams)\n",
    "def extract_ngrams(text, ngram_range=(1, 3)):\n",
    "    # Check if the text is valid (non-empty after removing spaces)\n",
    "    if text.strip():  \n",
    "        vectorizer = CountVectorizer(ngram_range=ngram_range, stop_words='english')\n",
    "        ngrams = vectorizer.fit_transform([text])\n",
    "        \n",
    "        # If no valid n-grams are found, return empty list\n",
    "        if ngrams.shape[1] > 0:\n",
    "            return vectorizer.get_feature_names_out()\n",
    "        else:\n",
    "            return ['<empty_ngram>']  # Return a placeholder for empty ngrams\n",
    "    else:\n",
    "        return ['<empty_ngram>']  # Return a placeholder for empty ngrams\n",
    "\n",
    "# Example lexicons for argument extraction (these could be refined further)\n",
    "agreement_lexicon = ['agree', 'agreed', 'agreement', 'yes', 'support']\n",
    "disagreement_lexicon = ['disagree', 'disagreed', 'disagreement', 'no', 'oppose']\n",
    "\n",
    "# Function to extract argument lexicons (agreement and disagreement)\n",
    "def extract_argument_lexicons(text):\n",
    "    agreement_count = sum([word in text.lower() for word in agreement_lexicon])\n",
    "    disagreement_count = sum([word in text.lower() for word in disagreement_lexicon])\n",
    "    return [agreement_count, disagreement_count]\n",
    "\n",
    "# Function to detect modal verbs (e.g., \"can\", \"could\", \"will\", etc.)\n",
    "def extract_modal_verbs(text):\n",
    "    doc = nlp(text)\n",
    "    modal_verbs = ['can', 'could', 'will', 'would', 'shall', 'should', 'may', 'might', 'must', 'ought']\n",
    "    modal_count = sum([token.lemma_ in modal_verbs for token in doc])\n",
    "    return [modal_count]\n",
    "\n",
    "# Function to detect negation words (e.g., \"not\", \"never\", etc.)\n",
    "def extract_negation(text):\n",
    "    negation_words = ['not', 'never', 'no', 'none', 'nothing', 'neither', 'nor']\n",
    "    negation_count = sum([word in text.lower() for word in negation_words])\n",
    "    return [negation_count]\n",
    "\n",
    "# Function to extract all features\n",
    "def extract_features(df):\n",
    "    ngram_features = []\n",
    "    other_features = []\n",
    "\n",
    "    # Loop through each text entry in the dataframe\n",
    "    for text in df['Text']:\n",
    "        # Extract n-grams (unigrams, bigrams, trigrams)\n",
    "        ngrams = extract_ngrams(text)\n",
    "        \n",
    "        # Extract additional features\n",
    "        argument_lexicons = extract_argument_lexicons(text)\n",
    "        modal_verbs = extract_modal_verbs(text)\n",
    "        negation = extract_negation(text)\n",
    "        \n",
    "        # Combine all features into a single list for this text\n",
    "        ngram_features.append(ngrams)\n",
    "        other_features.append(argument_lexicons + modal_verbs + negation)\n",
    "    \n",
    "    return ngram_features, other_features\n",
    "\n",
    "# Read the data from the CSV file\n",
    "df = pd.read_csv('compiled_output.csv')  # Replace with your file path\n",
    "\n",
    "# Check the first few rows of the dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Remove rows with NaN or empty Text after stripping extra spaces\n",
    "df = df[df['Text'].notna() & (df['Text'].str.strip() != '')]\n",
    "df['Text'] = df['Text'].str.strip()  # Remove leading/trailing spaces\n",
    "\n",
    "# Remove rows with empty or very short texts (length < 3 words)\n",
    "df['Text_Length'] = df['Text'].apply(lambda x: len(x.split()))\n",
    "df = df[df['Text_Length'] > 2]\n",
    "\n",
    "# Extract features and labels\n",
    "ngram_features, additional_features = extract_features(df)\n",
    "\n",
    "# Combine n-grams into one string for each row to pass into vectorizer\n",
    "ngram_texts = [' '.join(ngram) for ngram in ngram_features]\n",
    "\n",
    "# Create the CountVectorizer for n-grams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words='english')  # Added stop_words='english' here\n",
    "X_ngrams = vectorizer.fit_transform(ngram_texts)\n",
    "\n",
    "# Convert additional features to a numpy array\n",
    "X_additional = np.array(additional_features)\n",
    "\n",
    "# Combine n-grams and additional features (horizontal stack)\n",
    "X = hstack([X_ngrams, X_additional])\n",
    "\n",
    "# Prepare the target labels (Y)\n",
    "y = df['Label'].values\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
